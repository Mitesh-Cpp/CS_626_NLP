{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zijEpHvcAXea"
      },
      "outputs": [],
      "source": [
        "import nltk\n",
        "from nltk.corpus import brown\n",
        "nltk.download('brown')\n",
        "nltk.download('universal_tagset')\n",
        "tagged_sentences = brown.tagged_sents(tagset='universal')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5W26I3Q-AXeb"
      },
      "outputs": [],
      "source": [
        "# len(tagged_sentences)\n",
        "# print(tagged_sentences[0])\n",
        "tags = set()\n",
        "for sent in tagged_sentences:\n",
        "    for i in sent:\n",
        "        tags.add(i[1])\n",
        "print(tags)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "for ts in tagged_sentences:\n",
        "    for (w, t) in ts:\n",
        "        if '-' in w:\n",
        "            print(w, \" : \", t)"
      ],
      "metadata": {
        "id": "E-a-27AwQ9FH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MZ49wZC6AXeb"
      },
      "outputs": [],
      "source": [
        "from collections import defaultdict\n",
        "\n",
        "class HMMTagger:\n",
        "    def __init__(self, train_sentences, words, tags):\n",
        "        self.train_sentences = train_sentences\n",
        "        self.words = words\n",
        "        self.tags = tags\n",
        "        self.tagset = set(tags)\n",
        "        self.wordset = set(words)\n",
        "        self.initial_probs = {}\n",
        "        self.transition_probs = {}\n",
        "        self.emission_probs = {}\n",
        "\n",
        "    def train(self):\n",
        "        self._compute_initial_probs()\n",
        "        self._compute_transition_probs()\n",
        "        self._compute_emission_probs()\n",
        "\n",
        "    def _compute_initial_probs(self):\n",
        "        tag_counts = defaultdict(int)\n",
        "\n",
        "        for sentence in self.train_sentences:\n",
        "            if len(sentence) > 0:  # If the sentence is not empty\n",
        "                first_tag = sentence[0][1]\n",
        "                tag_counts[first_tag] += 1\n",
        "\n",
        "        total_sentences = len(self.train_sentences)\n",
        "\n",
        "        # Calculate initial probabilities\n",
        "        for tag in tag_counts:\n",
        "            self.initial_probs[tag] = tag_counts[tag] / total_sentences\n",
        "\n",
        "    def _compute_transition_probs(self):\n",
        "        bigram_counts = defaultdict(int)\n",
        "        tag_counts = defaultdict(int)\n",
        "\n",
        "        for sentence in self.train_sentences:\n",
        "            for i in range(len(sentence)):\n",
        "                tag_counts[sentence[i][1]] += 1\n",
        "                if i > 0:\n",
        "                    bigram_counts[(sentence[i - 1][1], sentence[i][1])] += 1\n",
        "\n",
        "        # Calculate transition probabilities\n",
        "        for tag1 in self.tagset:\n",
        "            for tag2 in self.tagset:\n",
        "                if tag_counts[tag1] > 0:\n",
        "                    self.transition_probs[(tag1, tag2)] = bigram_counts[(tag1, tag2)] / tag_counts[tag1]\n",
        "                else:\n",
        "                    self.transition_probs[(tag1, tag2)] = 0.0\n",
        "\n",
        "    def _compute_emission_probs(self):\n",
        "        word_tag_counts = defaultdict(int)\n",
        "        tag_counts = defaultdict(int)\n",
        "\n",
        "        # Calculate the frequency of (word, tag) pairs and individual tags\n",
        "        for w, t in zip(self.words, self.tags):\n",
        "            word_tag_counts[(w, t)] += 1\n",
        "            tag_counts[t] += 1\n",
        "\n",
        "        # Calculate emission probabilities\n",
        "        for (w, t) in word_tag_counts:\n",
        "            if tag_counts[t] > 0:\n",
        "                self.emission_probs[(t, w)] = word_tag_counts[(w, t)] / tag_counts[t]\n",
        "            else:\n",
        "                self.emission_probs[(t, w)] = 0.0\n",
        "\n",
        "    def _viterbi(self, phrase):\n",
        "        phrase = [word.lower().rstrip(\"'s\") if word.endswith(\"'s\") else word.lower() for word in phrase]\n",
        "        T = len(phrase)\n",
        "        N = len(self.tagset)\n",
        "        tags_list = list(self.tagset)\n",
        "\n",
        "        # Initialize the Viterbi table\n",
        "        V = [[0.0] * N for _ in range(T)]\n",
        "        backpointer = [[0] * N for _ in range(T)]\n",
        "\n",
        "        # Initialize the first row of the Viterbi table\n",
        "        for j in range(N):\n",
        "            tag = tags_list[j]\n",
        "            V[0][j] = self.initial_probs.get(tag, 1e-6) * self.emission_probs.get((tag, phrase[0]), 1e-6)  # Smoothing for unseen words\n",
        "            backpointer[0][j] = 0  # Start state has no previous state\n",
        "\n",
        "        # Fill in the rest of the Viterbi table\n",
        "        for i in range(1, T):\n",
        "            for j in range(N):\n",
        "                max_prob = -float('inf')\n",
        "                max_prev = 0\n",
        "                for k in range(N):\n",
        "                    prob = V[i - 1][k] * self.transition_probs.get((tags_list[k], tags_list[j]), 1e-6) * self.emission_probs.get((tags_list[j], phrase[i]), 1e-6)  # Smoothing for unseen words\n",
        "                    if prob > max_prob:\n",
        "                        max_prob = prob\n",
        "                        max_prev = k\n",
        "                V[i][j] = max_prob\n",
        "                backpointer[i][j] = max_prev\n",
        "\n",
        "        # Find the most likely final state\n",
        "        final_state = max(range(N), key=lambda j: V[T - 1][j])\n",
        "        max_prob = V[T - 1][final_state]\n",
        "\n",
        "        # Backtrack to find the most likely sequence of tags\n",
        "        result_tags = [tags_list[final_state]]\n",
        "        for i in range(T - 2, -1, -1):\n",
        "            final_state = backpointer[i + 1][final_state]\n",
        "            result_tags.insert(0, tags_list[final_state])\n",
        "\n",
        "        return result_tags"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TLD0npjkNU1x"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "from sklearn.model_selection import KFold\n",
        "from sklearn.metrics import confusion_matrix, classification_report\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "\n",
        "# Assuming you have already split your words and tags\n",
        "kf = KFold(n_splits=5)\n",
        "overall_confusion_matrix = np.zeros((12,12))\n",
        "tag_list = ['DET', 'PRT', 'ADV', 'X', 'CONJ', 'ADJ', 'ADP', 'PRON', 'NOUN', '.', 'NUM', 'VERB']\n",
        "\n",
        "all_y_true = []\n",
        "all_y_pred = []\n",
        "fold = 0\n",
        "\n",
        "for train_index, test_index in kf.split(tagged_sentences):\n",
        "    fold += 1\n",
        "    train_sentences = [tagged_sentences[i] for i in train_index]\n",
        "    test_sentences = [tagged_sentences[i] for i in test_index]\n",
        "\n",
        "    # creating words and tags list\n",
        "    words = []\n",
        "    tags = []\n",
        "    for sentence in train_sentences:\n",
        "        for word, tag in sentence:\n",
        "            words.append(word.lower().rstrip(\"'s\") if word.endswith(\"'s\") else word.lower())\n",
        "            tags.append(tag)\n",
        "\n",
        "    # Instantiate and train the model for this fold\n",
        "    model = HMMTagger(train_sentences, words, tags)\n",
        "    model.train()\n",
        "\n",
        "    y_true = []\n",
        "    y_pred = []\n",
        "\n",
        "    for sentence in test_sentences:\n",
        "        words_in_sentence = [word.lower() for word, tag in sentence]\n",
        "        true_tags = [tag for word, tag in sentence]\n",
        "\n",
        "        # Handle out-of-vocabulary (OOV) words inside _viterbi if necessary\n",
        "        predicted_tags = model._viterbi(words_in_sentence)\n",
        "\n",
        "        y_true.extend(true_tags)\n",
        "        y_pred.extend(predicted_tags)\n",
        "\n",
        "    # Add predictions of this fold to the overall lists\n",
        "    all_y_true.extend(y_true)\n",
        "    all_y_pred.extend(y_pred)\n",
        "\n",
        "    # Calculate and accumulate the confusion matrix for this fold\n",
        "    fold_confusion_matrix = confusion_matrix(y_true, y_pred, labels=tag_list)\n",
        "    overall_confusion_matrix += fold_confusion_matrix\n",
        "    print('-----------')\n",
        "    print(\"Fold: \", fold)\n",
        "    # Plot confusion matrix for the current fold\n",
        "    plt.figure(figsize=(10, 8))\n",
        "    sns.heatmap(fold_confusion_matrix, annot=True, fmt=\"d\", xticklabels=tag_list, yticklabels=tag_list)\n",
        "    plt.xlabel('Predicted')\n",
        "    plt.ylabel('True')\n",
        "    plt.title('Confusion Matrix - Fold')\n",
        "    plt.show()\n",
        "\n",
        "    print(classification_report(y_true, y_pred, labels=tag_list))\n",
        "    print('-----------')\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tYMte-SLPLWj"
      },
      "outputs": [],
      "source": [
        "print(\"Overall Accuracy\")\n",
        "\n",
        "print(classification_report(all_y_true, all_y_pred, labels=tag_list))\n",
        "\n",
        "\n",
        "plt.figure(figsize=(12, 10))\n",
        "overall_confusion_matrix = overall_confusion_matrix.astype('float') / overall_confusion_matrix.sum(axis=1)[:, np.newaxis]\n",
        "\n",
        "sns.heatmap(overall_confusion_matrix, annot=True, fmt=\".3f\",xticklabels=tag_list, yticklabels=tag_list)\n",
        "plt.xlabel('Predicted')\n",
        "plt.ylabel('True')\n",
        "plt.title('Overall Confusion Matrix Normalised')\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "G0kx7YewUSNJ"
      },
      "outputs": [],
      "source": [
        "def predict(sent):\n",
        "    sent  = sent.split()\n",
        "    pred = model._viterbi(sent)\n",
        "    return pred"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FqnNeq3JVaR5"
      },
      "outputs": [],
      "source": [
        "predict('Hello how are you')\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BZvhZ3sRVcJE"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "from sklearn.model_selection import KFold\n",
        "from sklearn.metrics import confusion_matrix, classification_report, fbeta_score\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "\n",
        "# Assuming you have already split your words and tags\n",
        "kf = KFold(n_splits=5)\n",
        "overall_confusion_matrix = np.zeros((12,12))\n",
        "tag_list = ['DET', 'PRT', 'ADV', 'X', 'CONJ', 'ADJ', 'ADP', 'PRON', 'NOUN', '.', 'NUM', 'VERB']\n",
        "\n",
        "all_y_true = []\n",
        "all_y_pred = []\n",
        "fold = 0\n",
        "\n",
        "for train_index, test_index in kf.split(tagged_sentences):\n",
        "    fold += 1\n",
        "    train_sentences = [tagged_sentences[i] for i in train_index]\n",
        "    test_sentences = [tagged_sentences[i] for i in test_index]\n",
        "\n",
        "    # creating words and tags list\n",
        "    words = []\n",
        "    tags = []\n",
        "    for sentence in train_sentences:\n",
        "        for word, tag in sentence:\n",
        "            words.append(word.lower().rstrip(\"'s\") if word.endswith(\"'s\") else word.lower())\n",
        "            tags.append(tag)\n",
        "\n",
        "    # Instantiate and train the model for this fold\n",
        "    model = HMMTagger(train_sentences, words, tags)\n",
        "    model.train()\n",
        "\n",
        "    y_true = []\n",
        "    y_pred = []\n",
        "\n",
        "    for sentence in test_sentences:\n",
        "        words_in_sentence = [word.lower() for word, tag in sentence]\n",
        "        true_tags = [tag for word, tag in sentence]\n",
        "\n",
        "        # Handle out-of-vocabulary (OOV) words inside _viterbi if necessary\n",
        "        predicted_tags = model._viterbi(words_in_sentence)\n",
        "\n",
        "        y_true.extend(true_tags)\n",
        "        y_pred.extend(predicted_tags)\n",
        "\n",
        "    # Add predictions of this fold to the overall lists\n",
        "    all_y_true.extend(y_true)\n",
        "    all_y_pred.extend(y_pred)\n",
        "\n",
        "    # Calculate and accumulate the confusion matrix for this fold\n",
        "    fold_confusion_matrix = confusion_matrix(y_true, y_pred, labels=tag_list)\n",
        "    overall_confusion_matrix += fold_confusion_matrix\n",
        "    print('-----------')\n",
        "    print(\"Fold: \", fold)\n",
        "    # Plot confusion matrix for the current fold\n",
        "    plt.figure(figsize=(10, 8))\n",
        "    sns.heatmap(fold_confusion_matrix, annot=True, fmt=\"d\", xticklabels=tag_list, yticklabels=tag_list)\n",
        "    plt.xlabel('Predicted')\n",
        "    plt.ylabel('True')\n",
        "    plt.title(f'Confusion Matrix - Fold {fold}')\n",
        "    plt.show()\n",
        "\n",
        "    print(classification_report(y_true, y_pred, labels=tag_list))\n",
        "    print('-----------')\n",
        "\n",
        "# After completing all folds, compute F0.5 and F2 for the entire set\n",
        "\n",
        "# Classification report for overall performance\n",
        "print(\"Overall Classification Report\")\n",
        "print(classification_report(all_y_true, all_y_pred, labels=tag_list))\n",
        "\n",
        "# F0.5 score\n",
        "f0_5 = fbeta_score(all_y_true, all_y_pred, beta=0.5, labels=tag_list, average='weighted')\n",
        "print(\"Overall F0.5 Score:\", f0_5)\n",
        "\n",
        "# F2 score\n",
        "f2 = fbeta_score(all_y_true, all_y_pred, beta=2, labels=tag_list, average='weighted')\n",
        "print(\"Overall F2 Score:\", f2)\n",
        "\n",
        "# Plot overall confusion matrix\n",
        "plt.figure(figsize=(12, 10))\n",
        "sns.heatmap(overall_confusion_matrix, annot=True, fmt=\"d\", xticklabels=tag_list, yticklabels=tag_list)\n",
        "plt.xlabel('Predicted')\n",
        "plt.ylabel('True')\n",
        "plt.title('Overall Confusion Matrix')\n",
        "plt.show()\n"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.8"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}